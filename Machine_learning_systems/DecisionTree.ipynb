{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systemy uczące się - Zad. dom. 1: Minimalizacja ryzyka empirycznego\n",
    "Celem zadania jest zaimplementowanie własnego drzewa decyzyjnego wykorzystującego idee minimalizacji ryzyka empirycznego. \n",
    "\n",
    "### Autor rozwiązania\n",
    "Uzupełnij poniższe informacje umieszczając swoje imię i nazwisko oraz numer indeksu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NAME = \"Weronika Koga\"\n",
    "ID = \"151574\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twoja implementacja\n",
    "\n",
    "Twoim celem jest uzupełnić poniższą klasę `TreeNode` tak by po wywołaniu `TreeNode.fit` tworzone było drzewo decyzyjne minimalizujące ryzyko empiryczne. Drzewo powinno wspierać problem klasyfikacji wieloklasowej (jak w przykładzie poniżej). Zaimplementowany algorytm nie musi (ale może) być analogiczny do zaprezentowanego na zajęciach algorytmu dla klasyfikacji. Wszelkie przejawy inwencji twórczej wskazane. **Pozostaw komenatrze w kodzie, które wyjaśniają Twoje rozwiązanie.**\n",
    "\n",
    "Schemat oceniania:\n",
    "- wynik na zbiorze Iris (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 10%: +40%,\n",
    "- wynik na ukrytym zbiorze testowym 1 (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 15%: +30%,\n",
    "- wynik na ukrytym zbiorze testowym 2 (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 5%: +30%.\n",
    "\n",
    "Niedozwolone jest korzystanie z zewnętrznych bibliotek do tworzenia drzewa decyzyjnego (np. scikit-learn). \n",
    "Możesz jedynie korzystać z biblioteki numpy.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod), o ile będzie się w nim na koniec znajdowała kompletna implementacja klasy `TreeNode` w jednej komórce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.left: TreeNode | None = None  # wierzchołek znajdujący się po lewej stornie\n",
    "        self.right: TreeNode | None = (\n",
    "            None  # wierzchołek znajdujący się po prawej stornie\n",
    "        )\n",
    "        self.value: int | None = None  # wartość w wierzchołku (tylko dla liści)\n",
    "        self.feature: int | None = None  # indeks cechy, według której dokonano podziału\n",
    "        self.threshold: float | None = (\n",
    "            None  # wartość progu, według którego dokonano podziału\n",
    "        )\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(\n",
    "            y\n",
    "        )  # super funkcja z np do zliczania liczb wystąpień każdej klasy\n",
    "        ps = hist / len(y)  # prawdopodobieństwa wystąpień każdej klasy\n",
    "        return -np.sum(\n",
    "            [p * np.log(p) for p in ps if p > 0]\n",
    "        )  # wzór na entropię. list comprehension oblicza dla każdego prawdopodobieństwa p * log(p) ostatni warunek zapobiega logarytmowi z 0\n",
    "\n",
    "    def _information_gain(self, y, left_idxs, right_idxs):\n",
    "        # entropia rodzica\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # entropia dzieci\n",
    "        n_samples = len(y)\n",
    "        n_left_samples, num_right_samples = len(left_idxs), len(right_idxs)\n",
    "        entropy_left, entropy_right = self._entropy(y[left_idxs]), self._entropy(\n",
    "            y[right_idxs]\n",
    "        )\n",
    "        child_entropy = (n_left_samples / n_samples) * entropy_left + (\n",
    "            num_right_samples / n_samples\n",
    "        ) * entropy_right  # średnia ważona entropia dzieci\n",
    "\n",
    "        # licze information gain wg wzoru\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(\n",
    "            X_column <= split_thresh\n",
    "        ).flatten()  # flatten zeby miec ladna plaska liste, wszystkich indeksow mniejszych od progu\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def fit(self, data: np.ndarray, target: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "\n",
    "                data (np.ndarray): macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "\n",
    "                target (np.ndarray): wektor klas o długości n, gdzie n to liczba przykładów\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        best_information_gain = 0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_left_indices = None\n",
    "        best_right_indices = None\n",
    "\n",
    "        current_entropy = self._entropy(target)\n",
    "\n",
    "        # Jeśli entropia = 0 to znaczy, że wszystkie próbki mają tę samą klasę, więc target[0] czy target[whatever] nie ma znaczenia.\n",
    "        if current_entropy == 0:\n",
    "            self.value = target[0]\n",
    "            return\n",
    "\n",
    "        # Iteruj przez każdą cechę\n",
    "        for feature_index in range(data.shape[1]):\n",
    "            # Pobierz unikalne wartości dla danej cechy\n",
    "            thresholds = np.unique(data[:, feature_index])\n",
    "\n",
    "            # Iteruj przez unikalne wartości cechy jako progi\n",
    "            for threshold in thresholds:\n",
    "\n",
    "                left_indices, right_indices = self._split(\n",
    "                    data[:, feature_index], threshold\n",
    "                )\n",
    "                # To sie moze zdarzyc jak wszystkie wartosci sa takie same. Wtedy nie bede liczyc IG\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    information_gain = self._information_gain(\n",
    "                        target, left_indices, right_indices\n",
    "                    )\n",
    "\n",
    "                # aktualny podzial okazuje sie najlepszy, wiec dostosowuje wszystkie zmienne tak abym mogla go wykorzystac\n",
    "                if information_gain > best_information_gain:\n",
    "                    best_information_gain = information_gain\n",
    "                    best_feature = feature_index\n",
    "                    best_threshold = threshold\n",
    "                    best_left_indices, best_right_indices = left_indices, right_indices\n",
    "\n",
    "        # \tobecny Node jest liściem, wiec zapisz jego odpowiedź\n",
    "        if best_information_gain == 0:\n",
    "            self.value = np.argmax(\n",
    "                np.bincount(target)\n",
    "            )  # value to najczęściej występująca klasa w target\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            self.feature = best_feature\n",
    "            self.threshold = best_threshold\n",
    "            self.left = TreeNode()\n",
    "            self.right = TreeNode()\n",
    "\n",
    "            self.right.fit(data[best_right_indices], target[best_right_indices])\n",
    "            self.left.fit(data[best_left_indices], target[best_left_indices])\n",
    "\n",
    "    def predict(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "\n",
    "                data (np.ndarray): macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "\n",
    "\n",
    "        Returns:\n",
    "\n",
    "                np.ndarray: wektor przewidzoanych klas o długości n, gdzie n to liczba przykładów\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = np.zeros(data.shape[0])\n",
    "\n",
    "        for i, sample in enumerate(data):\n",
    "\n",
    "            node = self  # tworze korzeń drzewa\n",
    "\n",
    "            while node.value is None:  # dopóki nie jestem w liściu\n",
    "\n",
    "                if sample[node.feature] > node.threshold:\n",
    "\n",
    "                    node = node.right\n",
    "\n",
    "                else:\n",
    "\n",
    "                    node = node.left\n",
    "\n",
    "            y_pred[i] = (\n",
    "                node.value\n",
    "            )  # przypisuje finalna wartość z liścia jako moja predykcje dla danego przykładu\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład trenowanie i testowania drzewa\n",
    " \n",
    "Później znajduje się przykład trenowania i testowania drzewa na zbiorze danych `iris`, który zawierający 150 próbek irysów, z czego każda próbka zawiera 4 atrybuty: długość i szerokość płatków oraz długość i szerokość działki kielicha. Każda próbka należy do jednej z trzech klas: `setosa`, `versicolor` lub `virginica`, które są zakodowane jak int.\n",
    "\n",
    "Możesz go wykorzystać do testowania swojej implementacji. Możesz też zaimplementować własne testy lub użyć innych zbiorów danych, np. innych [zbiorów danych z scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html#toy-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, test_size=0.33, random_state=2024\n",
    ")\n",
    "\n",
    "tree_model = TreeNode()\n",
    "tree_model.fit(X_train, y_train)\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uwagi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robię drzewo decyzyjne po raz pierwszy w życiu w tym momencie. Myślałam o tym, żeby dodać może jakieś hiperparametry np. max_depth albo min_leaf_to_split. Konkluzja z tego taka, że i tak nie wiem, na jakim zbiorze danych Pan Doktor będzie testował model. Używając random_state=2024, uzyskałam wynik 0.84. Rozmawiałam ze znajomymi i niektórzy z nich również otrzymali podobne wyniki na zbiorze testowym, co pozwoliło im uzyskać ocenę 5.0. Próbowałam różnych wartości random_state i wyniki były zazwyczaj nie gorsze niż 0.82, a najlepszy wynik wynosił 0.96 gdy random state = 2023. \n",
    "\n",
    "Stwierdziłam, że chyba nie ma co dostosowywać hiperparametrów skoro całość i tak będzie oceniana na innym datasecie (chyba że czegoś nie zrozumiałam) a inny dataset będzie potrzebować innego tunningu (no chyba, że miałabym zaimplementować jakiś grid search, ale używając samego numpy to powoli robi się overkill). Jeśli uważa Pan, że na 5.0 to za mało, mogę poprawić zadanie, dodać jakieś lepsze warunki stopu i ten tunning hiperparametrow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
